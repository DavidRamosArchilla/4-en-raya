{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Install kaggle-environments","metadata":{}},{"cell_type":"code","source":"# 1. Enable Internet in the Kernel (Settings side pane)\n\n# 2. Curl cache may need purged if v0.1.6 cannot be found (uncomment if needed). \n!curl -X PURGE https://pypi.org/simple/kaggle-environments\n\n# ConnectX environment was defined in v0.1.6\n!pip install 'kaggle-environments>=0.1.6'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-23T15:32:37.07041Z","iopub.execute_input":"2022-05-23T15:32:37.070715Z","iopub.status.idle":"2022-05-23T15:32:52.03671Z","shell.execute_reply.started":"2022-05-23T15:32:37.070658Z","shell.execute_reply":"2022-05-23T15:32:52.035839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create ConnectX Environment","metadata":{}},{"cell_type":"code","source":"from kaggle_environments import evaluate, make, utils\nimport numpy as np\nimport time\n\nenv = make(\"connectx\", debug=True)\nenv.render()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2022-05-23T15:32:52.039502Z","iopub.execute_input":"2022-05-23T15:32:52.039982Z","iopub.status.idle":"2022-05-23T15:32:52.241882Z","shell.execute_reply.started":"2022-05-23T15:32:52.039928Z","shell.execute_reply":"2022-05-23T15:32:52.241074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Mi Codigo","metadata":{}},{"cell_type":"code","source":"from abc import abstractmethod\n\nclass IGame:\n\n    @abstractmethod\n    def is_game_over(board, inarow):\n        pass\n\n    @abstractmethod\n    def change_turn(player):\n        pass\n\n    @abstractmethod\n    def get_open_cols(board):\n        pass\n\n    @abstractmethod\n    def make_move(board, col, player):\n        pass\n\n    @abstractmethod\n    def get_turn(board):\n        pass\n\n    @abstractmethod\n    def print_board(board):\n        pass","metadata":{"execution":{"iopub.status.busy":"2022-05-23T15:32:52.243249Z","iopub.execute_input":"2022-05-23T15:32:52.243525Z","iopub.status.idle":"2022-05-23T15:32:52.250669Z","shell.execute_reply.started":"2022-05-23T15:32:52.243482Z","shell.execute_reply":"2022-05-23T15:32:52.249921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\n\nclass Connectx(IGame):\n    def __init__(self, inarow, rows, cols):\n        self.cols = cols\n        self.rows = rows\n        self.inarow = inarow\n        \n\n    def change_turn(self, player): \n        if player == '1':\n            return '2'\n        else:\n            return '1'\n\n\n    def get_open_cols(self, board):\n        # board is serialized\n        return [i for i in range(self.cols) if board[i] == '0']\n\n    def make_move(self, board, col, player):\n\n        entire_col = [board[i*self.cols + col] for i in range(self.rows)]\n        for i, cell in enumerate(reversed(entire_col)):\n            if cell == '0':\n                board = board[:(self.rows-i-1)*self.cols + col] + player + board[(self.rows-i-1)*self.cols+col+1:]\n                break\n        return board\n\n    def get_turn(self, board):\n        # assuming that '1' always starts playing\n        ones = board.count('1')\n        twos = board.count('2')\n        return '1' if ones == twos else '2'\n\n    def is_game_over(self, board, inarow):\n        '''\n        return the winner or '0' if there is no winner yet\n        '''\n        board2d = self.deserialize_board(board)\n        \n        # rows\n        for i in range(self.rows):\n            previous = ''\n            count = 1\n            for j in range(self.cols):\n                current = board[i*self.cols + j]\n                if previous != '0' and previous == current:\n                    count += 1\n                else:\n                    count = 1\n                if count == inarow:\n                    return current\n                previous = current\n        \n        # columns\n        for i in range(self.cols):\n            previous = ''\n            count = 1\n            for j in range(self.rows):\n                current = board[j*self.cols + i]\n                if previous != '0' and previous == current:\n                    count += 1\n                else:\n                    count = 1\n                if count == inarow:\n                    return current\n                previous = current\n        \n        # positive diagonal\n        for row in range(self.rows-(inarow-1)):\n            for col in range(self.cols-(inarow-1)):\n                window = list(board2d[range(row, row+inarow), range(col, col+inarow)])\n                \n                if window.count('1') == inarow:\n                    return '1'\n                elif window.count('2') == inarow:\n                    return '2'\n\n        # negative diagonal\n        for row in range(inarow-1, self.rows):\n            for col in range(self.cols-(inarow-1)):\n                window = list(board2d[range(row, row-inarow, -1), range(col, col+inarow)])\n                if window.count('1') == inarow:\n                    return '1'\n                elif window.count('2') == inarow:\n                    return '2'\n        return '0' if board.count('0') != 0 else 'draw'\n\n    def print_board(self, board):\n        for i in range(self.rows):\n            print('|', end='')\n            for j in range(self.cols):\n                print(board[i*self.cols + j], end='|')\n            print()\n        print(board)\n    \n    def serialize_board(self, board):\n        return ''.join([str(cell) for row in board for cell in row])\n\n    def deserialize_board(self, board: str):\n        arr = np.array(list(board), dtype=np.int8)\n        arr = arr.reshape((self.rows, self.cols))\n        return arr\n    \n    def get_inputs(self, board, turn):\n        x = self.deserialize_board(board)\n        x1 = np.where(x==2, 1, 0)\n        x2 = np.where(x==1, 1, 0)\n        if turn == '2':\n            x3 = np.ones((self.rows, self.cols))\n        else:\n            x3 = np.zeros((self.rows, self.cols))\n        a = np.expand_dims(np.array([x1, x2, x3]), axis=0)\n        return a","metadata":{"execution":{"iopub.status.busy":"2022-05-23T15:32:52.252036Z","iopub.execute_input":"2022-05-23T15:32:52.252498Z","iopub.status.idle":"2022-05-23T15:32:52.28619Z","shell.execute_reply.started":"2022-05-23T15:32:52.252437Z","shell.execute_reply":"2022-05-23T15:32:52.285415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MCTS:\n\n    def __init__(self, game: IGame = None, model=None):\n        self.explored = set()\n        self.nodes_parameters = {} # fen: (N, V) N--> times visited, V-->value\n        # self.UCT = {} # fen: UPC (upper confidence tree)\n        self.C = 1.4 # aprox sqrt(2)\n        self.game = game\n        self.probabilities = {}\n        self.model = model\n\n    def get_value(self, result: str, player):\n        if result == '1':\n            return 1 if player == '1' else -1\n\n        elif result == '2':\n            return -1 if player == '1' else 1 \n        else:\n            return 0\n    # TODO: hay que cambiar la forma de explorar y/o seleccionar\n    def search(self, s):\n        result = self.game.is_game_over(s, self.game.inarow)\n        if result != '0':\n            v = self.get_value(result, self.game.get_turn(s)) \n            if s not in self.nodes_parameters:\n                self.nodes_parameters[s] = np.array((1, v))\n            else:\n                self.nodes_parameters[s][0] += 1\n            return -v, 1\n\n        childs = self.game.get_open_cols(s)\n\n        if s in self.explored:\n        # choose which node is going to be expanded\n            best_uct = float('-inf')\n            best_child = None\n            turn = self.game.get_turn(s)\n            # best_w = 0\n            n_p = self.nodes_parameters[s][0] # parent's n\n            for a in childs:\n                s_child = self.game.make_move(s, a, turn)\n                n, w = self.nodes_parameters[s_child]\n                p = self.probabilities[s_child]    \n                child_uct = self.get_UCT(n, w, n_p, p)\n                if child_uct > best_uct:\n                    best_uct = child_uct\n                    best_child = a\n            s_aux = self.game.make_move(s, best_child, self.game.get_turn(s))\n            sum_v, sum_n = self.search(s_aux)\n            # propagate the results\n            self.nodes_parameters[s_aux][0] += sum_n\n            self.nodes_parameters[s_aux][1] += sum_v\n\n        \n        else:\n            turn = self.game.get_turn(s)\n            if len(self.explored) == 0:\n                probs, v = self.model.predict(self.game.get_inputs(s, turn))\n                self.add_probs(s, probs)\n            self.explored.add(s)\n            sum_v = 0\n            sum_n = 0\n            for a in childs:\n                s_child = self.game.make_move(s, a, turn)\n                if s_child not in self.nodes_parameters:\n                    # v = self.simulate(s_child, turn, turn)\n                    probs, v = self.model.predict(self.game.get_inputs(s_child, turn))\n                    self.add_probs(s_child, probs)\n                    self.nodes_parameters[s_child] = np.array((1, v))\n                    sum_v += v\n                    sum_n += 1\n                \n            self.nodes_parameters[s][0] += sum_n\n            self.nodes_parameters[s][1] += sum_v\n\n        return -sum_v, sum_n\n    \n    def add_probs(self, state, probs):\n        open_cols = self.game.get_open_cols(state)\n        not_open_cols = [i for i in range(self.game.cols) if i not in open_cols]\n        probs = probs.reshape((self.game.cols,))\n        probs[not_open_cols] = 0\n        for a, prob in enumerate(np.ravel(probs)):\n            child_state = self.game.make_move(state, a, self.game.get_turn(state))\n            self.probabilities[child_state] = prob\n\n    def simulate(self, s, color_playing, turn):\n        result = self.game.is_game_over(s, self.game.inarow)\n        while result == '0': \n            move = np.random.choice(self.game.get_open_cols(s))\n            s = self.game.make_move(s, move, turn)\n            result = self.game.is_game_over(s, self.game.inarow)\n            turn = self.game.change_turn(turn)\n        return self.get_value(result, color_playing)\n\n\n    def get_UCT(self, n, w, n_p, p):\n        # return w/n + self.C * np.sqrt(n_p) / (1 + n)\n        return w/n + self.C * p * np.sqrt(np.log(n_p) / (n + 1))\n    \n    def get_pi(self, state, tau=1):\n        pi = []\n        for a in self.game.get_open_cols(state):\n            s_child = self.game.make_move(state, a, turn)\n            pi.append(self.nodes_parameters[s_child])\n        pi = np.power(np.array(pi), tau) \n        pi_sum = np.sum(pi)\n        return pi / pi_sum\n    \n    def iterate(self, s, n_iters=None, time_limit=None):\n        self.nodes_parameters[s] = np.array((1, 0))\n        if time_limit is not None:\n            end_time = time.time() + time_limit\n            while time.time() < end_time:\n                v, n = self.search(s)\n                self.nodes_parameters[s][0] += n\n                self.nodes_parameters[s][1] += v\n        else:\n            for _ in range(n_iters):\n                v, n = self.search(s)\n                self.nodes_parameters[s][0] += n\n                self.nodes_parameters[s][1] += v\n\n    def best_move(self, board, turn, n_iters=None, time_limit=None):\n        self.iterate(board, n_iters=n_iters, time_limit=time_limit)\n        max_n = 0\n        best_move = 3 # por ejemplo 3\n        for a in self.game.get_open_cols(board):\n            c_aux = self.game.make_move(board, a, turn)\n            if self.game.is_game_over(c_aux, self.game.inarow) != '0':\n                return a\n            current_n = self.nodes_parameters[c_aux][0]\n            if current_n > max_n:\n                max_n = current_n\n                best_move = a\n            # print(f'N: {self.nodes_parameters[c_aux][0]} , V: {self.nodes_parameters[c_aux][1]}')\n            # self.game.print_board(c_aux)\n        return best_move\n","metadata":{"execution":{"iopub.status.busy":"2022-05-23T15:36:54.101144Z","iopub.execute_input":"2022-05-23T15:36:54.101478Z","iopub.status.idle":"2022-05-23T15:36:54.143573Z","shell.execute_reply.started":"2022-05-23T15:36:54.10142Z","shell.execute_reply":"2022-05-23T15:36:54.142825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n# Neural Network","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, Conv2D, Dense, BatchNormalization, ReLU, Add, Flatten\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.optimizers import Adam\nimport tensorflow as tf\n\nphysical_devices = tf.config.experimental.list_physical_devices('GPU')\nif len(physical_devices) > 0:\n   tf.config.experimental.set_memory_growth(physical_devices[0], True)\n\nclass NeuralNetwok():\n    \n\n    def __init__(self, rows, cols, l_rate):\n        self.rows = rows\n        self.cols = cols\n        self.l_rate = l_rate\n        self.N_BLOCKS = 2\n        self.KERNEL_SIZE = (3, 3) # the stride will be 1 (the default used by keras)\n        self.FILTERS = 128\n        self.model = self.create_model(rows, cols, self.N_BLOCKS)\n        \n    \n    def create_model(self, rows, cols, n_blocks):\n        # input of the neural network\n        # 3 channels: one board with the position of th X's,\n        # other with the positions of the O's and the last, all 1 if X plays or all 0 if O plays\n        input_layer = Input(shape=(3, rows, cols))\n        \n        # body (residual blocks)\n        x = self.add_convolutional_layer(input_layer, self.FILTERS, self.KERNEL_SIZE)\n        for _ in range(n_blocks):\n            x = self.add_residual_block(x)\n            \n        # add policy head\n        policy_head_output = self.add_policy_head(x)\n        \n        # add value head\n        value_head_output = self.add_value_head(x)\n        \n        model = Model(inputs=input_layer, outputs=[policy_head_output, value_head_output])\n        model.compile(\n            optimizer = Adam(self.l_rate),\n            loss = ['categorical_crossentropy','mean_squared_error']\n        )\n        return model\n        \n    def add_residual_block(self, x):\n        # first convolutional layer\n        y = self.add_convolutional_layer(x, self.FILTERS, self.KERNEL_SIZE)\n        \n        # second convolutional layer\n        y = Conv2D(self.FILTERS, self.KERNEL_SIZE, data_format='channels_first', padding='same')(y)\n        y = BatchNormalization()(y)\n        y = Add()([x, y])\n        y = ReLU()(y)\n        return y\n    \n    def add_convolutional_layer(self, x, filters, kernel_size):\n        y = Conv2D(filters, kernel_size, data_format='channels_first', padding='same')(x)\n        # padding has to be 'same' in order to keep the dimensions right\n        # data_format=channels_first corresponds to inputs with shape (batch_size, channels, height, width).\n        y = BatchNormalization()(y)\n        y = ReLU()(y)\n        return y\n    \n    def add_policy_head(self, x):\n        y = self.add_convolutional_layer(x, 2, (1, 1))\n        y = Flatten()(y)\n        y = Dense(self.cols, activation='softmax')(y) # TODO: podria llegar a hacerse de tamaño rows*cols, ni idea\n        # move logit probabilities\n        return y\n    \n    def add_value_head(self, x):\n        y = self.add_convolutional_layer(x, 1, (1, 1))\n        y = Flatten()(y)\n        y = Dense(128)(y)\n        y = ReLU()(y)\n        y = Dense(1, activation='tanh')(y)\n        return y\n    \n    def predict(self, x):\n        return self.model.predict(x)\n    \n    # TODO: implementar\n    def train(self, x, y):\n        return\n        history = self.model.train()\n        \n    def save_model(self, file_name):\n        self.model.save(file_name)","metadata":{"execution":{"iopub.status.busy":"2022-05-23T15:34:44.493655Z","iopub.execute_input":"2022-05-23T15:34:44.49397Z","iopub.status.idle":"2022-05-23T15:34:44.517735Z","shell.execute_reply.started":"2022-05-23T15:34:44.493916Z","shell.execute_reply":"2022-05-23T15:34:44.516879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nn = NeuralNetwok(6, 7, 0.01)\n# plot_model(nn.model, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-23T15:34:45.562653Z","iopub.execute_input":"2022-05-23T15:34:45.562973Z","iopub.status.idle":"2022-05-23T15:34:47.637971Z","shell.execute_reply.started":"2022-05-23T15:34:45.562919Z","shell.execute_reply":"2022-05-23T15:34:47.637214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n\nclass Player:\n    \n    def __init__(self, mcts):\n        self.mcts = mcts\n        \n    def play(self):\n        for i in range(epochs):\n            neural_network = self.game.model\n            neural_network.save_model(f'model_iter_{i}.h5')\n            for j in range(num_games):\n                # TODO: acabar esto\n                examples += self.generate_examples()\n            x = \n            y = \n            neural_network.train(x, y)\n            old_network = load_model(f'model_iter_{i}.h5')\n            win_rate = self.simulate_games(old_network, 25) # TODO: usar variable global\n            \n            # if the trained network is not much better than the previous one, then the previous one remains as the current network\n            if win_rate < 0.55:\n                self.game.model = old_model\n            \n    def simulate_games(self, old_network, num_games):\n        old_mcts = MCTS()\n        old_mcts.game = self.mcts.game\n        old_mcts.model = old_network\n        num_wins = 0\n        for i in range(num_games):\n            color_old_network = '1' if np.random.uniform() else '2'\n            color_new_network = '2' if color_old_network == '1' else '1'\n            result = self.play_one_game(old_mcts, color_old_network)\n            if result == color_new_network:\n                num_wins += 1\n        return num_wins / num_games\n    \n    def generate_examples(self):\n        states = []\n        pi_s = []\n        results = []\n        board = '0' * (self.mcts.game.rows * self.mcts.game.cols)\n        result = '0'\n        current_turn = '1'\n        while result == '0':\n            move = self.mcts.best_move(board, rival_color, n_iters=40)\n            pi = self.mcts.get_pi(board)\n            board = self.mcts.game.make_move(board, move, current_turn)\n            \n            states.append(self.mcts.game.get_inputs(board, current_turn))\n            pi_s.append(pi)\n            result = self.mcts.game.is_game_over(board, self.mcts.game.inarow)\n            current_turn = self.mcts.game.change_turn(current_turn)\n        total_moves = len(states)\n        results = np.ones(total_moves)\n        if result == '1':\n            results[1::2] = -1\n        elif result == '2':\n            results[::2] = -1\n        else:\n            results = np.zeros(total)\n            \n        return (states, pi, results)\n        \n    def play_one_game(self, rival, rival_color):\n        board = '0' * (self.mcts.game.rows * self.mcts.game.cols)\n        result = '0'\n        is_rival_turn = rival_color == '1'\n        current_player_color = '2' if is_rival_turn else '1'\n        while result == '0':\n            if is_rival_turn:\n                move = rival.best_move(board, rival_color, n_iters=40) # TODO: usar variables globales para n_iters\n                board = self.mcts.game.make_move(board, move, rival_color)\n            else:\n                move = self.mcts.best_move(board, current_player_color, n_iters=40)\n                board = self.mcts.game.make_move(board, move, current_player_color)\n            is_rival_turn = not is_rival_turn\n            result = self.mcts.game.is_game_over(board, self.mcts.game.inarow)\n        return result     \n    \n    def result_to_int(result):\n        if result == '1':\n            return 1\n        elif result == '2':\n            return -1\n        else:\n            return 0","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\na = 3\nb = np.array([3, 3, 2])\nc = 0\na2 = 8\nb2 = np.array([3, 3, 2, 3, 3, 3, 3])\nc2 = 0\nar1 = np.array((a, b, c))\nnp.vstack((ar1, np.array((a2, b2, c2))))[:, 1]\nnp.power(np.array(b2), a) / a\nb2[1::2] = -1\nb2","metadata":{"execution":{"iopub.status.busy":"2022-05-25T20:49:26.943124Z","iopub.execute_input":"2022-05-25T20:49:26.943450Z","iopub.status.idle":"2022-05-25T20:49:26.953118Z","shell.execute_reply.started":"2022-05-25T20:49:26.943393Z","shell.execute_reply":"2022-05-25T20:49:26.952412Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"array([ 3, -1,  2, -1,  3, -1,  3])"},"metadata":{}}]},{"cell_type":"markdown","source":"# Create an Agent\n\nTo create the submission, an agent function should be fully encapsulated (no external dependencies).  \n\nWhen your agent is being evaluated against others, it will not have access to the Kaggle docker image.  Only the following can be imported: Python Standard Library Modules, gym, numpy, scipy, pytorch (1.3.1, cpu only), and more may be added later.\n\n","metadata":{}},{"cell_type":"code","source":"import functools\n# This agent random chooses a non-empty column.\n\ndef my_agent(observation, configuration):\n    board = functools.reduce(lambda x, y: str(x) + str(y), observation.board)\n    game = Connectx(configuration.inarow, configuration.rows, configuration.columns)\n    mcts = MCTS(game, nn)\n    turn = str(observation.mark)\n    \n#     print(turn, configuration.rows, configuration.columns)\n#     print(board)\n    game.print_board(board)\n    move = mcts.best_move(board, turn, n_iters=40)\n    for a in mcts.game.get_open_cols(board):\n        c_aux = mcts.game.make_move(board, a, turn)\n        print(f'N: {mcts.nodes_parameters[c_aux][0]} , V: {mcts.nodes_parameters[c_aux][1]}')\n#     print(move)\n    return move","metadata":{"execution":{"iopub.status.busy":"2022-05-23T15:39:01.147748Z","iopub.execute_input":"2022-05-23T15:39:01.148085Z","iopub.status.idle":"2022-05-23T15:39:01.15692Z","shell.execute_reply.started":"2022-05-23T15:39:01.14803Z","shell.execute_reply":"2022-05-23T15:39:01.155776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test your Agent","metadata":{}},{"cell_type":"code","source":"env.reset()\n# Play as the first agent against default \"random\" agent.\nenv.run([\"negamax\", my_agent])\nenv.render(mode=\"ipython\", width=500, height=450)","metadata":{"execution":{"iopub.status.busy":"2022-05-23T15:39:05.225754Z","iopub.execute_input":"2022-05-23T15:39:05.226128Z","iopub.status.idle":"2022-05-23T15:40:29.185309Z","shell.execute_reply.started":"2022-05-23T15:39:05.226078Z","shell.execute_reply":"2022-05-23T15:40:29.184522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Debug/Train your Agent","metadata":{}},{"cell_type":"code","source":"# Play as first position against random agent.\ntrainer = env.train([None, \"random\"])\n\nobservation = trainer.reset()\n\nwhile not env.done:\n    my_action = my_agent(observation, env.configuration)\n    print(\"My Action\", my_action)\n    observation, reward, done, info = trainer.step(my_action)\n    # env.render(mode=\"ipython\", width=100, height=90, header=False, controls=False)\nenv.render()","metadata":{"execution":{"iopub.status.busy":"2022-05-03T17:03:02.568414Z","iopub.execute_input":"2022-05-03T17:03:02.568885Z","iopub.status.idle":"2022-05-03T17:03:53.68903Z","shell.execute_reply.started":"2022-05-03T17:03:02.56883Z","shell.execute_reply":"2022-05-03T17:03:53.687988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate your Agent","metadata":{}},{"cell_type":"code","source":"def mean_reward(rewards):\n    return sum(r[0] for r in rewards) / float(len(rewards))\n\n# Run multiple episodes to estimate its performance.\nprint(\"My Agent vs Random Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"random\"], num_episodes=10)))\nprint(\"My Agent vs Negamax Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"negamax\"], num_episodes=10)))","metadata":{"execution":{"iopub.status.busy":"2022-04-20T21:27:33.205674Z","iopub.execute_input":"2022-04-20T21:27:33.205993Z","iopub.status.idle":"2022-04-20T21:47:17.80321Z","shell.execute_reply.started":"2022-04-20T21:27:33.205941Z","shell.execute_reply":"2022-04-20T21:47:17.801646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Play your Agent\nClick on any column to place a checker there (\"manually select action\").","metadata":{}},{"cell_type":"code","source":"# \"None\" represents which agent you'll manually play as (first or second player).\nenv.play([my_agent, None], width=500, height=450)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T17:06:22.740588Z","iopub.execute_input":"2022-05-03T17:06:22.740926Z","iopub.status.idle":"2022-05-03T17:06:31.408044Z","shell.execute_reply.started":"2022-05-03T17:06:22.740872Z","shell.execute_reply":"2022-05-03T17:06:31.407172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Write Submission File\n\n","metadata":{}},{"cell_type":"code","source":"import inspect\nimport os\n\ndef write_agent_to_file(function, file):\n    with open(file, \"a\" if os.path.exists(file) else \"w\") as f:\n        f.write(inspect.getsource(function))\n        print(function, \"written to\", file)\n\nwrite_agent_to_file(my_agent, \"submission.py\")","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:55:55.249739Z","iopub.execute_input":"2022-04-19T18:55:55.250318Z","iopub.status.idle":"2022-04-19T18:55:55.25931Z","shell.execute_reply.started":"2022-04-19T18:55:55.250264Z","shell.execute_reply":"2022-04-19T18:55:55.258129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validate Submission\nPlay your submission against itself.  This is the first episode the competition will run to weed out erroneous agents.\n\nWhy validate? This roughly verifies that your submission is fully encapsulated and can be run remotely.","metadata":{}},{"cell_type":"code","source":"# Note: Stdout replacement is a temporary workaround.\nimport sys\nout = sys.stdout\nsubmission = utils.read_file(\"/kaggle/working/submission.py\")\nagent = utils.get_last_callable(submission)\nsys.stdout = out\n\nenv = make(\"connectx\", debug=True)\nenv.run([agent, agent])\nprint(\"Success!\" if env.state[0].status == env.state[1].status == \"DONE\" else \"Failed...\")","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:58:39.674545Z","iopub.execute_input":"2022-04-19T18:58:39.674956Z","iopub.status.idle":"2022-04-19T18:58:39.697888Z","shell.execute_reply.started":"2022-04-19T18:58:39.67489Z","shell.execute_reply":"2022-04-19T18:58:39.696281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submit to Competition\n\n1. Commit this kernel.\n2. View the commited version.\n3. Go to \"Data\" section and find submission.py file.\n4. Click \"Submit to Competition\"\n5. Go to [My Submissions](https://kaggle.com/c/connectx/submissions) to view your score and episodes being played.","metadata":{}}]}